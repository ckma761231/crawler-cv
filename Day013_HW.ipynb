{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT 網路爬蟲實作練習\n",
    "\n",
    "\n",
    "* 能夠利用 Request + BeatifulSour 撰寫爬蟲，並存放到合適的資料結構\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "根據範例 ，完成以下問題：\n",
    "\n",
    "* ① 印出最新文章的「作者」「標題」「時間」\n",
    "* ② 印出第一頁所有文章的「作者」「標題」「時間」\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ① 印出最新文章的「作者」「標題」「時間」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "innocent8675 (坤溪戴維斯) Wed Dec 11 14:59:57 2019\n",
      "[花邊]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "r2 = requests.get('https://www.ptt.cc/bbs/NBA/index.html')\n",
    "\n",
    "soup2 = BeautifulSoup(r2.text, \"html5lib\")\n",
    "items = soup2.findAll(True, {\"class\": re.compile(r'(r-ent|r-list-sep)')})\n",
    "              #find(class_=\"r-list-container action-bar-margin bbs-screen\")\n",
    "for item in range(len(items)):\n",
    "    if 'r-list-sep' in items[item]['class']:\n",
    "        url_3 = 'https://www.ptt.cc' + items[item-1].find('a')['href']\n",
    "        #print(url_3)\n",
    "        r3 = requests.get(url_3)\n",
    "        soup3 = BeautifulSoup(r3.text)\n",
    "        inf = soup3.find_all(class_=\"article-meta-value\")\n",
    "        print(inf[0].text, inf[3].text)\n",
    "        print(inf[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ② 印出第一頁所有文章的「作者」「標題」「時間」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Your Code https://www.ptt.cc/bbs/NBA/M.1084672474.A.BFE.html\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.ptt.cc/bbs/NBA/index1.html'\n",
    "r = requests.get(url)\n",
    "\n",
    "soup0 = BeautifulSoup(r.text, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price (Price) Sun May 16 01:54:33 2004\n",
      "[轉錄]Lyotard 對於太陽板討論串的結論\n",
      "Price (Price) Mon May 24 00:43:00 2004\n",
      "[公告] 請停止一切關於本次活動的發言\n",
      "Frankaze (神采風華) Tue Jun  8 01:16:45 2004\n",
      "[轉錄]跟之前那篇比起來 我覺得這篇也應該轉過來\n",
      "Frankaze (神采風華) Tue Jun  8 09:02:23 2004\n",
      "[轉錄]再轉一篇好文來\n",
      "Price (Price) Thu Jun 10 00:39:35 2004\n",
      "[轉錄][情報] 夏洛特山貓系列\n",
      "Price (Price) Thu Jun 10 00:51:35 2004\n",
      "[轉錄]Re: [心得] 真是受不了糗爺....\n",
      "Price (Price) Thu Jun 10 12:05:07 2004\n",
      "[轉錄]總冠軍賽NO.2觀後感\n",
      "AmuroNamie (原來太嗨真的會失眠^___^) Fri Jun 11 15:06:09 2004\n",
      "[心得] Rasheed Wallace\n",
      "toptree (  ) Sat Jun 12 07:13:04 2004\n",
      "掌控球賽的男人\n",
      "skchang (3EB板開板囉！) Sat Jun 12 13:21:49 2004\n",
      "[閒聊] 說說2004季後賽名場面回顧\n",
      "shineup () Sat Jun 12 13:12:47 2004\n",
      "[心得] 其實說穿了 就是活塞的防守太可怕了\n",
      "cOvi (喵~) Sat Jun 12 16:43:07 2004\n",
      "Re: [閒聊] 說說2004季後賽名場面回顧\n",
      "ykshih (￼N￼Ns) Sat Jun 12 17:26:37 2004\n",
      "Re: [閒聊] 說說2004季後賽名場面回顧\n",
      "Frankaze (神采風華) Sat Jun 12 18:34:46 2004\n",
      "Re: [閒聊] 說說2004季後賽名場面回顧\n",
      "Price (Price) Mon Jun 14 02:18:52 2004\n",
      "[轉錄]Re: 總冠軍賽NO.3觀後感\n",
      "Frankaze (神采風華) Mon Jun 14 02:34:33 2004\n",
      "[轉錄]Re: 總冠軍賽NO.3觀後感\n",
      "star1 (第一次魔術表演不算成功) Mon Jun 14 07:16:30 2004\n",
      "Re: [轉錄]Re: 總冠軍賽NO.3觀後感\n",
      "coldspring (笑笑) Mon Jun 14 09:24:57 2004\n",
      "Re: [轉錄]Re: 總冠軍賽NO.3觀後感\n",
      "airbear (地圖) Mon Jun 14 12:56:14 2004\n",
      "Re: Kobe is frustrated...\n",
      "pennykidd (andre) Mon Jun 14 22:07:01 2004\n",
      "Re: [轉錄]Re: 總冠軍賽NO.3觀後感\n"
     ]
    }
   ],
   "source": [
    "for i in soup0.find(class_=\"r-list-container action-bar-margin bbs-screen\").find_all(class_=\"r-ent\"):\n",
    "    \n",
    "    try:\n",
    "        url_ = i.find(class_=\"title\").find('a')['href']#.text\n",
    "        url_0 = 'https://www.ptt.cc'+url_\n",
    "        r_1 = requests.get(url_0)\n",
    "        soup = BeautifulSoup(r_1.text, \"html5lib\")\n",
    "        r1 = soup.find_all(class_=\"article-meta-value\")\n",
    "        print(r1[0].text, r1[3].text)\n",
    "        print(r1[2].text)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ③ 試著爬爬看其他版的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Your Code\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.ptt.cc/bbs/Beauty/index.html'\n",
    "r = requests.get(url, cookies={'over18': '1'} )\n",
    "\n",
    "soup0 = BeautifulSoup(r.text, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daviddd (dd) Wed Dec 11 01:31:34 2019\n",
      "[正妹] 有東京夢的女孩\n",
      "ryanworld (涼粉) Wed Dec 11 01:44:55 2019\n",
      "[正妹] 日本女高校生四名\n",
      "lawyerchu (Bill) Wed Dec 11 06:51:16 2019\n",
      "[神人] 網路美女\n",
      "kelseyaya (Yaya) Wed Dec 11 09:23:01 2019\n",
      "[廣告] 極致的誘惑 鈴村愛里(鈴村あいり) #2\n",
      "ginlom (前世無情人，今生無女兒) Wed Dec 11 09:32:26 2019\n",
      "[正妹] 越南夢幻芭比\n",
      "g987669 (吃素一個月) Wed Dec 11 09:46:49 2019\n",
      "[正妹] 對岸的妹子\n",
      "Aa00111157 (九哥) Wed Dec 11 09:50:12 2019\n",
      "[正妹] 韓國的\n",
      "epili () Wed Dec 11 10:03:03 2019\n",
      "[正妹] 吃貨\n",
      "joanna1005 (喬安那rrrrr) Wed Dec 11 11:48:58 2019\n",
      "[正妹] 香港科大 台灣女孩 優秀人才\n",
      "graperson (葛培森) Wed Dec 11 11:58:08 2019\n",
      "[廣告] 中國的妹子\n",
      "sixx (SIX) Wed Dec 11 12:02:44 2019\n",
      "[正妹] 泰國youtuber\n",
      "ReiKuromiya (Rei Kuromiya) Wed Dec 11 12:15:08 2019\n",
      "[正妹] =籃球五年級\n",
      "riyan (riri) Wed Dec 11 12:54:07 2019\n",
      "[正妹] Max S（大S、小S）\n",
      "AmedRosario (SS) Wed Dec 11 13:09:01 2019\n",
      "[正妹] 甜美可愛的\n",
      "xuanyuan (軒轅氏) Wed Dec 11 14:01:51 2019\n",
      "[神人]  2012台灣第一女總統抽籤場\n",
      "hateOnas (△怕老婆△怕老婆) Tue May 21 13:19:08 2019\n",
      "[公告] 不願上表特 ＆ 優文推薦 ＆ 檢舉建議專區\n",
      "hateOnas (△怕老婆△怕老婆) Wed Jul 24 17:34:05 2019\n",
      "Fw: [公告] 請使用者多加注意我國保護兒少的法令\n",
      "hateOnas (△怕老婆△怕老婆) Fri Jul 26 12:21:17 2019\n",
      "[公告] 表特板板規(2019.7.26)\n",
      "hateOnas (△怕老婆△怕老婆) Fri Jul 26 13:04:09 2019\n",
      "[公告] 201907 板主徵選延長\n",
      "hateOnas (△怕老婆△怕老婆) Sat Nov 23 20:42:59 2019\n",
      "[公告] 請勿意淫推文\n"
     ]
    }
   ],
   "source": [
    "for i in soup0.find(class_=\"r-list-container action-bar-margin bbs-screen\").find_all(class_=\"r-ent\"):\n",
    "    \n",
    "    try:\n",
    "        url_ = i.find(class_=\"title\").find('a')['href']#.text\n",
    "        url_0 = 'https://www.ptt.cc'+url_\n",
    "        r_1 = requests.get(url_0, cookies={'over18': '1'})\n",
    "        soup = BeautifulSoup(r_1.text, \"html5lib\")\n",
    "        r1 = soup.find_all(class_=\"article-meta-value\")\n",
    "        print(r1[0].text, r1[3].text)\n",
    "        print(r1[2].text)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://www.ptt.cc/bbs/NBA/index.html'\n",
    "url2 = 'https://www.ptt.cc/bbs/NBA/index1.html'\n",
    "\n",
    "r = requests.get(url1)\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html5lib\")\n",
    "\n",
    "columns = ['作者', '看板', '標題', '時間']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
